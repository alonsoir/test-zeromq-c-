# PROMPT DE CONTINUIDAD - DÃA 34 (06 Enero 2026)

## ğŸ¯ BACKLOG Management Protocol

**Al completar cualquier tarea mayor, Claude debe:**

1. âœ… Confirmar completion con Alonso
2. ğŸ“‹ Solicitar acceso al BACKLOG.md actualizado
3. ğŸ” Revisar prioridades actuales (P0 â†’ P1 â†’ P2 â†’ P3)
4. ğŸ’¡ Sugerir siguiente tarea basÃ¡ndose en:
    - Blockers crÃ­ticos (P0)
    - Dependencies del roadmap
    - Estado de Foundation Architecture
    - Effort vs Impact ratio
5. ğŸ¤ Esperar aprobaciÃ³n de Alonso antes de proceder

**Frase trigger para Claude:**
> "Tarea completada. Â¿Puedo ver el BACKLOG.md para sugerir quÃ© sigue?"

**PriorizaciÃ³n actual (Ene 2026):**
- P0 BLOCKER: ISSUE-005 (JSONL memory leak) â† CURRENT
- P1 HIGH: FAISS Integration (blocked by ISSUE-005)
- P1 HIGH: BACKLOG-001 Flow Sharding (post-FAISS)
- P2 MEDIUM: etcd-client, Watcher, Academic paper

**Via Appia Quality reminder:**
> Resolver blockers antes que features.  
> Foundation sÃ³lida antes que expansiÃ³n.  
> Memory leaks son P0, no P2.

## ğŸ“š DOCUMENTOS NECESARIOS PARA ESTA SESIÃ“N
```
Day 34 (HOY):
  âŒ NO pasar FAISS_ANTI_CURSE_DESIGN.md
  âœ… Solo este prompt de continuidad
  
RazÃ³n: Day 34 es testing con datos reales JSONL.
       No implementamos estrategias anti-curse todavÃ­a.
       El resumen abajo es suficiente.

RECORDATORIO PARA DÃAS FUTUROS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Day 35 (DimensionalityReducer):                    â”‚
â”‚   âœ… PASAR FAISS_ANTI_CURSE_DESIGN.md              â”‚
â”‚   RazÃ³n: Implementar Estrategia #2 (PCA)          â”‚
â”‚                                                     â”‚
â”‚ Day 36 (Ãndices Separados + Selective):           â”‚
â”‚   âœ… PASAR FAISS_ANTI_CURSE_DESIGN.md              â”‚
â”‚   RazÃ³n: Implementar Estrategias #1 y #3          â”‚
â”‚                                                     â”‚
â”‚ Day 38-40 (Advanced Strategies):                   â”‚
â”‚   âœ… PASAR FAISS_ANTI_CURSE_DESIGN.md              â”‚
â”‚   RazÃ³n: Temporal Tiers, Re-ranking, etc.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Archivo: /vagrant/docs/FAISS_ANTI_CURSE_DESIGN.md
TamaÃ±o: ~500 lÃ­neas (12K tokens aprox)
```

---

## ğŸ“‹ CONTEXTO DÃA 33 (05 Enero 2026) - COMPLETADO âœ…

### âœ… Real ONNX Embedder Models - Complete

**Day 33 - Embedder Models Created:**
- âœ… create_chronos_embedder.py: 83â†’512-d time series embedder
- âœ… create_sbert_embedder.py: 83â†’384-d semantic embedder
- âœ… create_attack_embedder.py: 83â†’256-d attack pattern embedder
- âœ… test_embedders.py: Verification suite (3/3 tests PASSED âœ…)
- âœ… All models exported to ONNX (opset 18)
- âœ… Git commit: Scripts committed, models excluded (.gitignore)

**Models Generated (Day 33):**
```
âœ… chronos_embedder.onnx - 13KB (83â†’512-d)
âœ… sbert_embedder.onnx   - 22KB (83â†’384-d)
âœ… attack_embedder.onnx  - 9.7KB (83â†’256-d)
```

**Infrastructure Status (Days 31-33 Complete):**
```
âœ… FAISS v1.8.0 - WORKING
   â”œâ”€ test_faiss_basic PASSING
   â”œâ”€ CV computation validated
   â””â”€ Auto-detection working

âœ… ONNX Runtime v1.17.1 - WORKING
   â”œâ”€ test_onnx_basic PASSING
   â”œâ”€ Inference pipeline validated
   â””â”€ Auto-detection working

âœ… ONNX Embedder Models - CREATED
   â”œâ”€ Chronos (time series): 83â†’512-d âœ…
   â”œâ”€ SBERT (semantic): 83â†’384-d âœ…
   â”œâ”€ Attack (patterns): 83â†’256-d âœ…
   â””â”€ All verified with onnx.checker âœ…

âœ… Build System - ROBUST
   â”œâ”€ CMakeLists.txt: C++20, auto-detect
   â”œâ”€ Makefile: test-faiss, test-onnx, test-all
   â””â”€ All targets working

âœ… Strategic Design - PEER REVIEWED
   â”œâ”€ FAISS_ANTI_CURSE_DESIGN.md v2.0
   â”œâ”€ 11 estrategias definidas
   â”œâ”€ Peer review: 4 AI systems
   â””â”€ LÃ­mites empÃ­ricamente validados
```

**Test Results (Day 33):**
```
make test-all              â†’ ALL TESTS PASSED âœ…
python3 test_embedders.py  â†’ 3/3 models verified âœ…
```

**Git Status:**
```
Rama: feature/faiss-ingestion-phase2a
Ãšltimo commit: "Day 33: Real ONNX embedder models created"
Archivos aÃ±adidos:
  - rag/models/create_chronos_embedder.py
  - rag/models/create_sbert_embedder.py
  - rag/models/create_attack_embedder.py
  - rag/models/test_embedders.py
  - rag/models/.gitignore
  - .gitguardian.yaml (updated)
```

**Via Appia Quality Achievement (Day 33):**
> "Creamos modelos sintÃ©ticos con arquitectura correcta para validar
> el pipeline HOY. Los modelos reales son future work. Pipeline
> validation > Model perfection. Tiempo: 2.5h de 4-6h estimadas.
> Despacio, pero avanzando. ğŸ›ï¸"

---

## ğŸ”¬ RESUMEN ESTRATEGIAS ANTI-CURSE (Para Referencia Day 34)

**Estrategias que implementaremos Days 35+:**

### ğŸ”´ CRÃTICAS - Phase 2A (Days 35-38)

**1. Ãndices Separados por Clase** (Day 36)
- Benign index vs Malicious index
- 10x mejora para Attack embedder
- Evita saturaciÃ³n cross-class

**2. Dimensionality Reduction Post-Embedding** (Day 35)
- **CRÃTICO**: Usar faiss::PCAMatrix (NO Eigen manual)
- 512â†’128 (preserva 96.8% varianza), 384â†’96, 256â†’64
- 4x mejora en lÃ­mites
- **Necesitaremos FAISS_ANTI_CURSE_DESIGN.md en Day 35**

**3. Selective Embedding** (Day 36)
- Malicious: 100% embedded
- Benign: 10% sampling (hash determinista)
- 10x mejora para clase benign

### ğŸŸ¡ IMPORTANTES - Phase 2B (Days 38-40)

**4. Temporal Tiers** (Day 39)
- Hot (7 dÃ­as): ~700 eventos, CV > 0.3
- Warm (30 dÃ­as): IVF, CV > 0.2
- Cold (30+ dÃ­as): IVF+PQ, compressed

**5. Metadata-First Search** (Day 38)
- Pre-filter con SQL/etcd
- FAISS solo para refinamiento

**6. Quantization** (Day 40)
- float32 â†’ int8 (4x compresiÃ³n)
- <1% pÃ©rdida precisiÃ³n

### ğŸ”µ AVANZADAS - Qwen Contributions

**9. IVF Attack-Aware** (Day 39)
**10. Two-Stage Re-ranking** (Day 38)
**11. Cold Start Strategy** (Day 35)

**LÃ­mites EmpÃ­ricamente Validados:**
```
Chronos (512-d â†’ 128-d): 180K eventos (CV = 0.20)
SBERT (384-d â†’ 96-d):    450K eventos (CV = 0.20)
Attack (256-d â†’ 64-d):   85K benign (CV = 0.20)
```

---

## ğŸ¯ ESTADO ACTUAL - DÃA 34 INICIO

### âœ… Completado Hasta Ahora

**Phase 2A Infrastructure (Days 31-33):**
- âœ… FAISS v1.8.0 instalado, testeado, working
- âœ… ONNX Runtime v1.17.1 instalado, testeado, working
- âœ… Build system configurado (C++20, auto-detection)
- âœ… Tests pasando (test_faiss_basic, test_onnx_basic)
- âœ… Anti-curse design completado (v2.0, peer-reviewed)
- âœ… **3 embedder models ONNX creados y verificados** ğŸ‰

**Modelos Disponibles:**
- âœ… chronos_embedder.onnx (13KB) - Time series
- âœ… sbert_embedder.onnx (22KB) - Semantic
- âœ… attack_embedder.onnx (9.7KB) - Attack patterns
- âœ… Todos verificados con onnx.checker
- âœ… Scripts en git, modelos regenerables

**Datos Disponibles:**
- âœ… 32,957 eventos RAG (JSONL format)
- âœ… 43,526 artifacts Protobuf
- âœ… 43,526 artifacts JSON
- âŒ NO tenemos embeddings pre-computados (.npy)
- âœ… Modelos listos para generar embeddings

### ğŸš§ Pendiente - Week 5

**Day 34 (HOY): Test Embedders con Datos Reales** â† ESTAMOS AQUÃ
- Load eventos JSONL (~32,957 disponibles)
- Extract 83 features por evento
- Run inference a travÃ©s de 3 embedders
- Verify output shapes y distribuciones
- Test con ONNX Runtime C++

**Days 35-40: Implementation**
- DimensionalityReducer (faiss::PCAMatrix) â† **PASAR DESIGN DOC**
- AttackIndexManager (Ã­ndices separados) â† **PASAR DESIGN DOC**
- SelectiveEmbedder (sampling) â† **PASAR DESIGN DOC**
- ChunkCoordinator integration
- End-to-end pipeline

---

## ğŸš€ PLAN DÃA 34 - TEST CON DATOS REALES

### ğŸ¯ Objetivo del DÃ­a

**Focus**: Test los 3 embedders ONNX con eventos JSONL reales, validar pipeline end-to-end.

**Contexto Importante:**
- âœ… Tenemos 3 modelos ONNX funcionando
- âœ… Tenemos ~32,957 eventos JSONL disponibles
- âœ… ONNX Runtime v1.17.1 instalado y testeado
- ğŸ¯ Objetivo: Probar inference pipeline completo

**Timeline**: 2-3 horas total

**Status**: Models Created âœ… â†’ Test with Real Data (Day 34) â†’ DimensionalityReducer (Day 35)

---

### FASE 1: Python Inference Pipeline (1.5 horas)

**Objetivo**: Cargar eventos JSONL â†’ Extract features â†’ Generate embeddings

**Script: test_real_inference.py**
```python
#!/usr/bin/env python3
"""
Test ONNX embedders with real ML Defender events.

Process:
1. Load events from JSONL
2. Extract 83 features per event
3. Run inference through all 3 embedders
4. Verify output shapes and distributions
"""

import json
import numpy as np
import onnxruntime as ort
from pathlib import Path
from datetime import datetime

def load_events_jsonl(jsonl_path, max_events=100):
    """Load events from JSONL file"""
    events = []
    with open(jsonl_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= max_events:
                break
            try:
                event = json.loads(line.strip())
                events.append(event)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  Skipping line {i}: {e}")
                continue
    return events

def extract_features(event):
    """
    Extract 83 features from RAG event.
    
    Features (83 total):
    - Timestamp features: 7 (year, month, day, hour, minute, second, microsecond)
    - IP features: 8 (src_ip octets Ã— 4, dst_ip octets Ã— 4)
    - Port features: 2 (src_port, dst_port)
    - Protocol features: 3 (protocol, ip_version, tcp_flags)
    - Packet features: 4 (packet_length, header_length, payload_length, ttl)
    - Detection scores: 5 (fast_score, ml_score, final_score, is_malicious, severity)
    - Network metadata: 6 (vlan_id, dscp, ecn, window_size, mss, seq_num)
    - Behavioral features: 48 (flow stats, timing, patterns)
    """
    features = np.zeros(83, dtype=np.float32)
    
    # Timestamp features (0-6)
    if 'timestamp' in event:
        ts = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
        features[0] = ts.year / 2026.0  # Normalize
        features[1] = ts.month / 12.0
        features[2] = ts.day / 31.0
        features[3] = ts.hour / 24.0
        features[4] = ts.minute / 60.0
        features[5] = ts.second / 60.0
        features[6] = ts.microsecond / 1e6
    
    # IP features (7-14)
    if 'src_ip' in event:
        octets = [int(x) for x in event['src_ip'].split('.')]
        features[7:11] = np.array(octets) / 255.0
    if 'dst_ip' in event:
        octets = [int(x) for x in event['dst_ip'].split('.')]
        features[11:15] = np.array(octets) / 255.0
    
    # Port features (15-16)
    features[15] = event.get('src_port', 0) / 65535.0
    features[16] = event.get('dst_port', 0) / 65535.0
    
    # Protocol features (17-19)
    features[17] = event.get('protocol', 0) / 255.0
    features[18] = event.get('ip_version', 4) / 6.0
    features[19] = event.get('tcp_flags', 0) / 255.0
    
    # Packet features (20-23)
    features[20] = min(event.get('packet_length', 0) / 65535.0, 1.0)
    features[21] = event.get('header_length', 0) / 255.0
    features[22] = min(event.get('payload_length', 0) / 65535.0, 1.0)
    features[23] = event.get('ttl', 64) / 255.0
    
    # Detection scores (24-28)
    features[24] = event.get('fast_detector_score', 0.0)
    features[25] = event.get('ml_detector_score', 0.0)
    features[26] = event.get('final_score', 0.0)
    features[27] = float(event.get('is_malicious', False))
    features[28] = event.get('severity', 0) / 10.0
    
    # Network metadata (29-34)
    features[29] = event.get('vlan_id', 0) / 4095.0
    features[30] = event.get('dscp', 0) / 63.0
    features[31] = event.get('ecn', 0) / 3.0
    features[32] = event.get('window_size', 0) / 65535.0
    features[33] = event.get('mss', 0) / 65535.0
    features[34] = min(event.get('seq_num', 0) / 4294967295.0, 1.0)
    
    # Behavioral features (35-82) - Placeholder
    # In production, these would include:
    # - Flow statistics (bytes/packets sent/received)
    # - Timing features (inter-arrival times, duration)
    # - Pattern features (entropy, periodicity, burstiness)
    # For now, fill with reasonable defaults
    for i in range(35, 83):
        features[i] = 0.5  # Neutral value
    
    return features

def test_embedder(model_path, features, model_name):
    """Test a single embedder with features"""
    print(f"\n{'='*60}")
    print(f"Testing: {model_name}")
    print('='*60)
    
    # Load model
    print("Step 1: Loading ONNX model...")
    session = ort.InferenceSession(model_path)
    print(f"  âœ… Model loaded: {model_path}")
    
    # Get input/output names
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    print(f"  Input: {input_name}, Output: {output_name}")
    
    # Run inference
    print("\nStep 2: Running inference...")
    input_data = features.reshape(1, -1).astype(np.float32)
    outputs = session.run([output_name], {input_name: input_data})
    embedding = outputs[0]
    
    print(f"  âœ… Inference completed")
    print(f"  Input shape: {input_data.shape}")
    print(f"  Output shape: {embedding.shape}")
    
    # Validate output
    print("\nStep 3: Validating output...")
    print(f"  Embedding dimension: {embedding.shape[1]}")
    print(f"  Value range: [{embedding.min():.4f}, {embedding.max():.4f}]")
    print(f"  Mean: {embedding.mean():.4f}, Std: {embedding.std():.4f}")
    
    # Show first few values
    print(f"  First 5 values: {' '.join(f'{v:.4f}' for v in embedding[0][:5])}")
    
    return embedding

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ML Defender - Real Data Inference Test              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # Find latest JSONL file
    data_dir = Path("/vagrant/data/rag/events")
    jsonl_files = sorted(data_dir.glob("*.jsonl"))
    
    if not jsonl_files:
        print("âŒ No JSONL files found in /vagrant/data/rag/events")
        return 1
    
    latest_jsonl = jsonl_files[-1]
    print(f"ğŸ“„ Using JSONL file: {latest_jsonl.name}")
    print(f"   Path: {latest_jsonl}\n")
    
    # Load events
    print("Step 1: Loading events...")
    events = load_events_jsonl(latest_jsonl, max_events=10)
    print(f"  âœ… Loaded {len(events)} events\n")
    
    if not events:
        print("âŒ No events loaded")
        return 1
    
    # Extract features from first event
    print("Step 2: Extracting features from first event...")
    features = extract_features(events[0])
    print(f"  âœ… Extracted {len(features)} features")
    print(f"  Feature range: [{features.min():.4f}, {features.max():.4f}]")
    print(f"  First 10 features: {' '.join(f'{v:.3f}' for v in features[:10])}\n")
    
    # Test all embedders
    models = [
        ("chronos_embedder.onnx", "Chronos (Time Series)", 512),
        ("sbert_embedder.onnx", "SBERT (Semantic)", 384),
        ("attack_embedder.onnx", "Attack (Patterns)", 256),
    ]
    
    results = []
    for model_path, model_name, expected_dim in models:
        try:
            embedding = test_embedder(model_path, features, model_name)
            
            # Verify dimension
            actual_dim = embedding.shape[1]
            if actual_dim == expected_dim:
                print(f"  âœ… Dimension correct: {actual_dim}")
                results.append((model_name, "âœ… PASS"))
            else:
                print(f"  âŒ Dimension mismatch: expected {expected_dim}, got {actual_dim}")
                results.append((model_name, "âŒ FAIL"))
        except Exception as e:
            print(f"\nâŒ {model_name} FAILED: {e}")
            results.append((model_name, f"âŒ ERROR: {e}"))
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    for model, status in results:
        print(f"  {model:30s} {status}")
    
    print("\n" + "="*60)
    passed = sum(1 for _, status in results if status.startswith("âœ…"))
    print(f"Result: {passed}/{len(models)} embedders passed")
    print("="*60)
    
    if passed == len(models):
        print("\nâœ… ALL EMBEDDERS WORKING WITH REAL DATA")
        return 0
    else:
        print("\nâŒ SOME TESTS FAILED")
        return 1

if __name__ == "__main__":
    exit(main())
```

**Ejecutar:**
```bash
cd /vagrant/rag/models
python3 test_real_inference.py
```

---

### FASE 2: C++ Inference Test (1 hora)

**Objetivo**: Adaptar test_onnx_basic.cpp para usar nuestros modelos

**File: rag/tests/test_real_embedders.cpp**
```cpp
#include <iostream>
#include <vector>
#include <fstream>
#include <onnxruntime_cxx_api.h>

// Test one embedder with 83 features
void test_embedder(const char* model_path, const char* name, size_t expected_dim) {
    std::cout << "\n" << std::string(60, '=') << "\n";
    std::cout << "Testing: " << name << "\n";
    std::cout << std::string(60, '=') << "\n";
    
    // Initialize ONNX Runtime
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "ml-defender");
    Ort::SessionOptions session_options;
    
    // Load model
    std::cout << "Step 1: Loading model...\n";
    Ort::Session session(env, model_path, session_options);
    std::cout << "  âœ… Model loaded: " << model_path << "\n";
    
    // Prepare 83 features (dummy values for now)
    std::vector<float> input_data(83, 0.5f);  // All 0.5 as placeholder
    
    // Create input tensor
    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    std::vector<int64_t> input_shape = {1, 83};
    
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, 
        input_data.data(), 
        input_data.size(),
        input_shape.data(), 
        input_shape.size()
    );
    
    // Run inference
    std::cout << "\nStep 2: Running inference...\n";
    const char* input_names[] = {"features"};
    const char* output_names[] = {"embedding"};
    
    auto output_tensors = session.Run(
        Ort::RunOptions{nullptr},
        input_names, &input_tensor, 1,
        output_names, 1
    );
    
    // Validate output
    float* output_data = output_tensors[0].GetTensorMutableData<float>();
    auto output_shape = output_tensors[0].GetTensorTypeAndShapeInfo().GetShape();
    
    std::cout << "  âœ… Inference completed\n";
    std::cout << "  Output shape: [" << output_shape[0] << ", " << output_shape[1] << "]\n";
    std::cout << "  Expected dim: " << expected_dim << "\n";
    
    if (output_shape[1] == expected_dim) {
        std::cout << "  âœ… Dimension correct\n";
    } else {
        std::cout << "  âŒ Dimension mismatch!\n";
        throw std::runtime_error("Dimension mismatch");
    }
    
    // Show first few values
    std::cout << "  First 5 values: ";
    for (size_t i = 0; i < 5; ++i) {
        std::cout << output_data[i] << " ";
    }
    std::cout << "\n";
}

int main() {
    std::cout << "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n";
    std::cout << "â•‘  ML Defender - C++ Embedder Test                      â•‘\n";
    std::cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
    
    try {
        test_embedder("chronos_embedder.onnx", "Chronos (Time Series)", 512);
        test_embedder("sbert_embedder.onnx", "SBERT (Semantic)", 384);
        test_embedder("attack_embedder.onnx", "Attack (Patterns)", 256);
        
        std::cout << "\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n";
        std::cout << "â•‘  ALL TESTS PASSED âœ…                                   â•‘\n";
        std::cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        
        return 0;
    } catch (const std::exception& e) {
        std::cerr << "\nâŒ TEST FAILED: " << e.what() << "\n";
        return 1;
    }
}
```

**Compilar y ejecutar:**
```bash
cd /vagrant/rag/models
g++ -std=c++20 -o test_real_embedders test_real_embedders.cpp \
    -I/usr/local/include \
    -L/usr/local/lib \
    -lonnxruntime

./test_real_embedders
```

---

### FASE 3: Batch Processing Test (0.5 horas)

**Objetivo**: Procesar mÃºltiples eventos y medir performance
```python
#!/usr/bin/env python3
"""
Batch processing test for embedders.

Tests:
- Load 100 events
- Generate embeddings for all
- Measure throughput
- Check consistency
"""

import time
import numpy as np
import onnxruntime as ort
from test_real_inference import load_events_jsonl, extract_features
from pathlib import Path

def batch_process(model_path, features_batch, batch_size=10):
    """Process features in batches"""
    session = ort.InferenceSession(model_path)
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    
    embeddings = []
    num_batches = (len(features_batch) + batch_size - 1) // batch_size
    
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(features_batch))
        batch = features_batch[start_idx:end_idx]
        
        # Pad batch if needed
        if len(batch) < batch_size:
            padding = np.zeros((batch_size - len(batch), 83), dtype=np.float32)
            batch = np.vstack([batch, padding])
        
        # Run inference
        outputs = session.run([output_name], {input_name: batch.astype(np.float32)})
        embeddings.append(outputs[0][:len(batch)])
    
    return np.vstack(embeddings) if embeddings else np.array([])

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ML Defender - Batch Processing Test                 â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # Load events
    data_dir = Path("/vagrant/data/rag/events")
    jsonl_files = sorted(data_dir.glob("*.jsonl"))
    latest_jsonl = jsonl_files[-1]
    
    print(f"ğŸ“„ Loading events from: {latest_jsonl.name}\n")
    events = load_events_jsonl(latest_jsonl, max_events=100)
    print(f"  âœ… Loaded {len(events)} events\n")
    
    # Extract all features
    print("Extracting features...")
    features_list = []
    for i, event in enumerate(events):
        if i % 20 == 0:
            print(f"  Progress: {i}/{len(events)}")
        features = extract_features(event)
        features_list.append(features)
    
    features_batch = np.array(features_list)
    print(f"  âœ… Extracted features: {features_batch.shape}\n")
    
    # Test each embedder
    models = [
        ("chronos_embedder.onnx", "Chronos"),
        ("sbert_embedder.onnx", "SBERT"),
        ("attack_embedder.onnx", "Attack"),
    ]
    
    for model_path, name in models:
        print(f"\n{'='*60}")
        print(f"Testing: {name}")
        print('='*60)
        
        # Warm-up
        _ = batch_process(model_path, features_batch[:10], batch_size=10)
        
        # Benchmark
        start = time.time()
        embeddings = batch_process(model_path, features_batch, batch_size=10)
        elapsed = time.time() - start
        
        throughput = len(features_batch) / elapsed
        
        print(f"  âœ… Processed {len(features_batch)} events")
        print(f"  Time: {elapsed:.2f}s")
        print(f"  Throughput: {throughput:.1f} events/sec")
        print(f"  Embedding shape: {embeddings.shape}")
        print(f"  Mean: {embeddings.mean():.4f}, Std: {embeddings.std():.4f}")
    
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  BATCH PROCESSING COMPLETE âœ…                          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

if __name__ == "__main__":
    main()
```

---

## âœ… CRITERIOS DE Ã‰XITO DÃA 34
```
1. Python Inference:
   âœ… test_real_inference.py created
   âœ… Load events from JSONL
   âœ… Extract 83 features correctly
   âœ… All 3 embedders produce valid outputs
   âœ… Dimensions correct (512, 384, 256)
   
2. C++ Inference:
   âœ… test_real_embedders.cpp created
   âœ… Compile and run successfully
   âœ… ONNX Runtime integration working
   âœ… All 3 embedders tested
   
3. Batch Processing:
   âœ… Process 100+ events
   âœ… Measure throughput
   âœ… Verify consistency

4. Documentation:
   âœ… Update DAY34_SUMMARY.md
   âœ… Document feature extraction logic
```

---

## ğŸ“… TIMELINE - SEMANA 5 (ACTUALIZADO)
```
âœ… Day 31: FAISS + Anti-curse design
âœ… Day 32: ONNX Runtime test
âœ… Day 33: Real embedders (3 ONNX models) âœ…

ğŸ”¥ Day 34: Test con datos reales (2-3h) â† ESTAMOS AQUÃ
   - Load eventos JSONL
   - Extract 83 features
   - Run inference (Python + C++)
   - Batch processing test
   âŒ NO necesita FAISS design doc

ğŸ“… Day 35: DimensionalityReducer (6h)
   âœ… PASAR FAISS_ANTI_CURSE_DESIGN.md â† IMPORTANTE
   - Implement faiss::PCAMatrix
   - Train PCA (10K eventos)
   - Test reduction pipeline
   - 512â†’128, 384â†’96, 256â†’64

ğŸ“… Day 36-38: Integration (8h)
   âœ… PASAR FAISS_ANTI_CURSE_DESIGN.md â† IMPORTANTE
   - AttackIndexManager
   - SelectiveEmbedder
   - ChunkCoordinator
   - End-to-end tests
```

---

## ğŸš€ COMANDOS RÃPIDOS DÃA 34
```bash
# Verificar modelos existentes
cd /vagrant/rag/models
ls -lh *.onnx

# Fase 1: Python inference (1.5h)
python3 test_real_inference.py

# Fase 2: C++ inference (1h)
# [Crear test_real_embedders.cpp]
g++ -std=c++20 -o test_real_embedders test_real_embedders.cpp \
    -I/usr/local/include -L/usr/local/lib -lonnxruntime
./test_real_embedders

# Fase 3: Batch processing (0.5h)
python3 test_batch_processing.py

# Git
cd /vagrant
git add rag/models/test_real_inference.py
git add rag/models/test_real_embedders.cpp
git add rag/models/test_batch_processing.py
git commit -m "Day 34: Test embedders with real JSONL data"
```

---

## ğŸ›ï¸ VIA APPIA QUALITY - FILOSOFÃA DAY 34

> "Day 33 creamos modelos sintÃ©ticos. Day 34 los validamos con datos
> reales. 32,957 eventos JSONL disponibles. Extract 83 features por
> evento. Test inference Python + C++. Medir throughput. Objetivo:
> confirmar que pipeline funciona end-to-end antes de implementar
> PCA y estrategias anti-curse. ValidaciÃ³n antes de optimizaciÃ³n.
> Despacio, pero avanzando. ğŸ›ï¸"

**Key Principle:**
- âœ… Validation before optimization
- âœ… Real data before synthetic only
- âœ… Python + C++ both working
- âœ… Throughput measurement important

---

**Next**: Day 34 - Test con datos reales â†’ Day 35 - DimensionalityReducer (**+ PASAR DESIGN DOC**)

**Via Appia Quality**: Validar pipeline con datos reales antes de optimizar. Despacio y bien. ğŸ›ï¸