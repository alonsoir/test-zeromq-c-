#include "rag/llama_integration.hpp"
#include <llama.h>
#include <vector>
#include <string>
#include <iostream>
#include <sstream>

class LlamaIntegration::Impl {
private:
    llama_model* model = nullptr;
    llama_context* ctx = nullptr;
    bool model_loaded = false;

    // ðŸŽ¯ **FUNCIÃ“N ALTERNATIVA PARA LIMPIAR CACHE KV**
    void clear_kv_cache() {
        // Crear un batch vacÃ­o para forzar reset del estado interno
        llama_batch batch = llama_batch_init(1, 0, 1);
        batch.n_tokens = 0;  // Batch vacÃ­o

        // Esto resetea el estado interno del contexto
        llama_decode(ctx, batch);
        llama_batch_free(batch);

        std::cout << "ðŸ§¹ Cache KV resetado manualmente" << std::endl;
    }

public:
    bool loadModel(const std::string& model_path) {
        std::cout << "ðŸ”„ Cargando modelo REAL: " << model_path << std::endl;

        try {
            // Inicializar backend
            llama_backend_init();

            // ParÃ¡metros del modelo
            llama_model_params model_params = llama_model_default_params();
            model = llama_model_load_from_file(model_path.c_str(), model_params);

            if (!model) {
                std::cerr << "âŒ ERROR: No se pudo cargar el modelo desde: " << model_path << std::endl;
                std::cerr << "   Verifica que el archivo existe y es un modelo GGUF vÃ¡lido" << std::endl;
                return false;
            }

            std::cout << "âœ… Modelo cargado exitosamente: " << model_path << std::endl;

            // ParÃ¡metros del contexto
            llama_context_params ctx_params = llama_context_default_params();
            ctx_params.n_ctx = 1024;
            ctx_params.n_threads = 2;

            ctx = llama_init_from_model(model, ctx_params);
            if (!ctx) {
                std::cerr << "âŒ ERROR: No se pudo crear el contexto" << std::endl;
                llama_model_free(model);
                model = nullptr;
                return false;
            }

            model_loaded = true;
            std::cout << "ðŸš€ Sistema LLM REAL listo para generar respuestas" << std::endl;
            return true;

        } catch (const std::exception& e) {
            std::cerr << "âŒ EXCEPCIÃ“N durante carga del modelo: " << e.what() << std::endl;
            return false;
        }
    }

    std::string generateResponse(const std::string& prompt, int max_tokens = 128) {
        if (!model_loaded) {
            return "âŒ Error: Modelo no cargado correctamente";
        }

        std::cout << "ðŸŽ¯ Generando respuesta REAL para: \"" << prompt << "\"" << std::endl;

        try {
            // ðŸŽ¯ **SOLUCIÃ“N: LIMPIAR CACHE KV MANUALMENTE ANTES DE CADA CONSULTA**
            clear_kv_cache();

            const llama_vocab* vocab = llama_model_get_vocab(model);

            // MEJORAR EL PROMPT PARA CONTEXTO DE SEGURIDAD
            std::string enhanced_prompt =
                "<|system|>\n"
                "Eres un asistente especializado en seguridad informÃ¡tica y anÃ¡lisis de comandos de Linux. "
                "Responde de manera concisa y profesional. Analiza comandos peligrosos y proporciona recomendaciones de seguridad.\n"
                "<|user|>\n" + prompt + "\n"
                "<|assistant|>\n";

            // Tokenizar el prompt mejorado
            std::vector<llama_token> tokens;
            tokens.resize(enhanced_prompt.size() + 16);

            int n_tokens = llama_tokenize(vocab, enhanced_prompt.c_str(), enhanced_prompt.length(),
                                         tokens.data(), tokens.size(), true, false);

            if (n_tokens <= 0) {
                return "âŒ Error: No se pudo tokenizar el prompt";
            }

            tokens.resize(n_tokens);
            std::cout << "ðŸ“Š Tokens de entrada: " << tokens.size() << std::endl;

            // Configurar batch - ðŸŽ¯ **INICIALIZAR CORRECTAMENTE POSICIONES**
            llama_batch batch = llama_batch_init(512, 0, 1);
            batch.n_tokens = n_tokens;

            for (int i = 0; i < n_tokens; i++) {
                batch.token[i] = tokens[i];
                batch.pos[i] = i;  // ðŸŽ¯ **SIEMPRE EMPEZAR EN 0 PARA NUEVA CONSULTA**
                batch.seq_id[i][0] = 0;
                batch.n_seq_id[i] = 1;
                batch.logits[i] = (i == n_tokens - 1);
            }

            // Evaluar prompt inicial
            std::cout << "ðŸ” Realizando decodificaciÃ³n inicial..." << std::endl;
            int ret = llama_decode(ctx, batch);

            if (ret != 0) {
                llama_batch_free(batch);
                return "âŒ Error en decodificaciÃ³n inicial: " + std::to_string(ret);
            }

            std::stringstream response;
            int tokens_generated = 0;
            const int n_vocab = llama_vocab_n_tokens(vocab);

            // GeneraciÃ³n de tokens
            for (int i = 0; i < max_tokens; i++) {
                float* logits = llama_get_logits_ith(ctx, -1);

                // Sampling greedy
                llama_token new_token = 0;
                float max_logit = logits[0];
                for (int j = 1; j < n_vocab; j++) {
                    if (logits[j] > max_logit) {
                        max_logit = logits[j];
                        new_token = j;
                    }
                }

                // Verificar token EOS
                if (new_token == llama_vocab_eos(vocab)) {
                    std::cout << "ðŸ”š Token EOS detectado" << std::endl;
                    break;
                }

                // Convertir token a texto
                char piece[32];
                int n_chars = llama_token_to_piece(vocab, new_token, piece, sizeof(piece), 0, false);
                if (n_chars < 0) break;

                if (n_chars > 0) {
                    response << std::string(piece, n_chars);
                }

                // Preparar siguiente token - ðŸŽ¯ **POSICIÃ“N CONSECUTIVA**
                llama_batch next_batch = llama_batch_init(1, 0, 1);
                next_batch.n_tokens = 1;
                next_batch.token[0] = new_token;
                next_batch.pos[0] = n_tokens + i;  // ðŸŽ¯ **POSICIÃ“N CONSECUTIVA CORRECTA**
                next_batch.seq_id[0][0] = 0;
                next_batch.n_seq_id[0] = 1;
                next_batch.logits[0] = true;

                ret = llama_decode(ctx, next_batch);
                llama_batch_free(next_batch);

                if (ret != 0) {
                    std::cout << "âš ï¸  Error en decodificaciÃ³n de token " << i << ": " << ret << std::endl;
                    break;
                }

                tokens_generated++;
            }

            // Liberar batch principal
            llama_batch_free(batch);

            std::string result = response.str();
            std::cout << "âœ… GeneraciÃ³n REAL completada: " << tokens_generated << " tokens generados" << std::endl;
            return result.empty() ? "ðŸ¤– [El modelo no generÃ³ respuesta]" : result;

        } catch (const std::exception& e) {
            std::cerr << "âŒ Error durante generaciÃ³n: " << e.what() << std::endl;
            return "âš ï¸  Error en generaciÃ³n";
        }
    }

    ~Impl() {
        if (ctx) {
            llama_free(ctx);
            ctx = nullptr;
        }
        if (model) {
            llama_model_free(model);
            model = nullptr;
        }
        llama_backend_free();
    }
};

// Implementaciones wrapper
LlamaIntegration::LlamaIntegration() : pImpl(std::make_unique<Impl>()) {}
LlamaIntegration::~LlamaIntegration() = default;

bool LlamaIntegration::loadModel(const std::string& model_path) {
    return pImpl->loadModel(model_path);
}

std::string LlamaIntegration::generateResponse(const std::string& prompt) {
    return pImpl->generateResponse(prompt);
}