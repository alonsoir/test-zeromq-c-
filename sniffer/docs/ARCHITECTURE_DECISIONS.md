# ML Defender - Architectural Decisions
## Phase 1, Day 2: Feature Extraction Architecture

**Date:** November 17, 2025  
**Status:** Implemented  
**Philosophy:** Via Appia - Build to last decades, not days

---

## Executive Summary

This document explains the architectural decisions made for the ML Defender feature extraction system, particularly the choice to implement all 40 features in userspace rather than splitting them between kernel (eBPF) and userspace.

**Key Decision:** All 40 features calculated in USERSPACE (Option A)

**Future Work:** Explore kernel pre-computation optimization (Option B) after establishing performance baseline

---

## Context: The Problem

ML Defender implements 4 embedded C++20 RandomForest detectors:
1. DDoS Detection (10 features)
2. Ransomware Detection (10 features)
3. Traffic Classification (10 features)
4. Internal Anomaly Detection (10 features)

**Total: 40 features**

These features are derived from synthetic datasets based on statistical distributions, fundamentally different from the previous CICIDS2017-based 83 features (which we maintain separately for scientific honesty research).

---

## Decision: Userspace-Only Feature Extraction

### The Options Evaluated

We evaluated three architectural approaches:

#### Option A: All Features in Userspace âœ… (SELECTED)
```
eBPF/XDP â†’ Raw packet capture
    â†“
Userspace â†’ FlowStatistics aggregation
    â†“
Userspace â†’ All 40 features calculated
    â†“
Userspace â†’ ML inference (RandomForest)
    â†“
Kernel â†’ Firewall action
```

**Pros:**
- Simple, clean architecture
- Easy to debug and profile
- No premature optimization
- Fast development iteration
- All features accessible for refinement

**Cons:**
- Potentially higher CPU usage (to be measured)
- More context switches (to be measured)

#### Option B: Hybrid Kernel/Userspace (FUTURE)
```
eBPF/XDP â†’ Packet capture + basic counters
    â†“         (SYN count, ACK count, bytes, etc.)
Userspace â†’ Aggregate counters + complex math
    â†“         (entropy, ratios, statistics)
Userspace â†’ ML inference
    â†“
Kernel â†’ Firewall action
```

**Pros:**
- Potentially lower latency (to be validated)
- Reduced data transfer to userspace

**Cons:**
- More complex architecture
- Harder to debug
- eBPF limitations (no floating point, limited stack)
- Requires careful division of labor

#### Option C: Maximum Kernel Optimization (REJECTED)
Try to calculate features in kernel (eBPF).

**Why Rejected:**
- eBPF cannot do floating-point math
- Statistical calculations (entropy, std dev) too complex for kernel
- Synthetic dataset features inherently require temporal aggregation
- Would violate eBPF verifier constraints

---

## Rationale: Why Option A Now, Option B Later

### 1. Nature of Synthetic Dataset Features

Our features are fundamentally **statistical distributions**:
- Shannon entropy
- Standard deviations
- Coefficients of variation
- Inter-arrival time statistics
- Temporal pattern analysis

These require:
- **Temporal windows** (not single packets)
- **Aggregated data** (multiple packets per flow)
- **Complex mathematics** (logarithms, square roots, division)

**Conclusion:** These features naturally belong in userspace where we have full computational capabilities and can maintain flow state.

### 2. Via Appia Engineering Philosophy

> "First make it work, then make it right, then make it fast"
> â€” Kent Beck

**Our approach:**
1. âœ… **Make it work** (Phase 1): Implement all features in userspace
2. ðŸ“Š **Measure** (Phase 1): Establish performance baseline
3. ðŸ“ˆ **Analyze** (Phase 2): Identify actual bottlenecks
4. âš¡ **Optimize** (Phase 2): Move proven bottlenecks to kernel if beneficial

**Anti-pattern we avoided:**
Premature optimization based on assumptions rather than measurements.

### 3. Scientific Honesty

By implementing Option A first:
- We establish a **performance baseline** with real measurements
- We can publish **honest numbers** about what works
- We can later publish **comparative analysis** of Option A vs Option B
- We avoid claiming benefits we haven't proven

**Paper contribution:** "We evaluated both approaches and measured their trade-offs in production"

### 4. Separation of Concerns

**Critical Path** (sub-microsecond requirement):
```
Packet â†’ Features â†’ ML â†’ Block Decision
```

**Analysis Path** (human-speed, seconds OK):
```
Blocked IPs â†’ GeoIP â†’ Threat Intel â†’ Dashboard â†’ RAG
```

We keep these **strictly separated**. Features needed for blocking stay in critical path. Context (like GeoIP) is deferred to analysis path.

---

## Implementation Status: Phase 1

### Features Implemented (22/40)

**Fully Implemented:**
- DDoS: syn_ack_ratio, packet_symmetry, protocol_anomaly_score, packet_size_entropy, traffic_amplification_factor, flow_completion_rate, traffic_escalation_rate, resource_saturation_score (8/10)

- Ransomware: entropy, network_activity, temporal_pattern, access_frequency, data_volume, behavior_consistency (6/10)

- Traffic: packet_rate, avg_packet_size, temporal_consistency (3/10)

- Internal: protocol_regularity, packet_size_consistency, data_exfiltration_indicators, temporal_anomaly_score, access_pattern_entropy (5/10)

**Total: 22 features with real implementations**

### Features with Honest TODOs (18/40)

**Features requiring multi-flow aggregator:**
- source_ip_dispersion (DDoS)
- connection_rate (Traffic)
- tcp_udp_ratio (Traffic)
- port_entropy (Traffic)
- flow_duration_std (Traffic)
- src_ip_entropy (Traffic)
- dst_ip_concentration (Traffic)
- protocol_variety (Traffic)
- internal_connection_rate (Internal)
- service_port_consistency (Internal)
- connection_duration_std (Internal)
- lateral_movement_score (Internal)
- service_discovery_patterns (Internal)

**Features requiring system-level metrics:**
- io_intensity (Ransomware)
- resource_usage (Ransomware)
- process_anomaly (Ransomware)
- file_operations (Ransomware)

**Features requiring GeoIP (deliberately deferred):**
- geographical_concentration (DDoS)

Each TODO includes:
- Clear explanation of why it's a TODO
- What data/infrastructure is needed
- Suggested implementation approach
- Expected behavior for Phase 1 (neutral value 0.5)

---

## Special Case: GeoIP and the Critical Path

### The Problem

Geographic IP location provides valuable context:
- "Where is the attack coming from?"
- "Are attacks concentrated in certain regions?"
- "Is this consistent with known threat actors?"

However, GeoIP lookups introduce **unacceptable latency**:
- REST API calls: 100-500ms
- Local MaxMind DB: 1-10ms
- Even cached lookups: >1ms

**Our latency budget:** ~10 microseconds total

**Conclusion:** GeoIP in critical path would blow our budget by 100-50,000x.

### The Solution: Two-Tier Architecture

#### Tier 1: Critical Path (sub-microsecond)
```
Packet â†’ Features â†’ ML â†’ Block Decision
          â†“
    NO GeoIP here!
```

**Question:** "Is this traffic an attack?"  
**Data needed:** Flow patterns, not geography  
**Latency:** Sub-microsecond

#### Tier 2: Analysis Path (human-speed)
```
Blocked IPs â†’ RAG Query â†’ GeoIP Service â†’ Context
                â†“
          "Attacks from China, Russia"
          "Coincides with APT28 patterns"
```

**Question:** "Where did the attack come from?"  
**Data needed:** Geographic context  
**Latency:** Seconds (acceptable)

### Why This Works

**Information Theory Insight:**
Geographic location is **USEFUL** but not **NECESSARY** for attack detection.

A SYN flood is a SYN flood whether it originates from:
- China
- Russia
- USA
- Mars

The **attack pattern** determines if we block, not the **geographic origin**.

**Analogy (explained to grandma):**
"If someone is throwing rocks at your window, you close the shutters immediately. You don't wait to find out which country they're from before protecting yourself."

---

## Performance Expectations

### Current Implementation (Option A)

**Estimated latency breakdown:**
```
eBPF capture:         ~1 Î¼s
Userspace receive:    ~1 Î¼s
Feature extraction:   ~5 Î¼s  (22 features)
ML inference:         ~3 Î¼s  (RandomForest)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:               ~10 Î¼s
```

**To be measured in Phase 2 with production traffic.**

### Future Optimization (Option B)

**Theoretical latency with kernel pre-computation:**
```
eBPF capture:         ~1 Î¼s
eBPF counters:        ~1 Î¼s  (SYN, ACK, bytes, etc.)
Userspace math:       ~3 Î¼s  (entropy, ratios)
ML inference:         ~3 Î¼s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                ~8 Î¼s (20% improvement)
```

**To be validated when we implement Option B.**

---

## Code Organization

### Separation from CICIDS2017 Code

```
sniffer/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ feature_extractor.hpp         # 83 CICIDS features (paper on bias)
â”‚   â””â”€â”€ ml_defender_features.hpp      # 40 new features (production)
â”‚
â””â”€â”€ src/userspace/
    â”œâ”€â”€ feature_extractor.cpp          # CICIDS implementation
    â””â”€â”€ ml_defender_features.cpp       # ML Defender implementation
```

**Philosophy:**
- Keep CICIDS code intact for scientific paper on dataset bias
- Completely separate ML Defender code for production
- Easy to deprecate CICIDS in v1.0
- No mixing of concerns

**Benefits:**
- Can A/B test both systems
- Can publish comparative analysis
- Clean migration path to production
- Easy to maintain two codebases temporarily

---

## Future Work: Phase 2 Roadmap

### 1. Performance Baseline (Week 1)
- [ ] Deploy Phase 1 implementation to production
- [ ] Measure actual latency with real traffic
- [ ] Profile CPU usage per feature
- [ ] Identify bottlenecks (if any)

### 2. Multi-Flow Aggregator (Week 2-3)
- [ ] Implement FlowAggregator component
- [ ] Track multiple flows in temporal windows
- [ ] Implement 13 multi-flow features
- [ ] Measure latency impact

### 3. Kernel Optimization Exploration (Week 4-5)
- [ ] Implement Option B (kernel pre-computation)
- [ ] Benchmark Option A vs Option B
- [ ] Measure actual performance gains
- [ ] Cost/benefit analysis

### 4. System Metrics Integration (Week 6)
- [ ] Add eBPF tracepoints for I/O operations
- [ ] Collect CPU/memory metrics
- [ ] Implement 4 system-level features
- [ ] Validate ransomware detection improvement

### 5. Publication (Month 3)
- [ ] Paper: "Architectural Trade-offs in Sub-Microsecond IDS"
- [ ] Compare Option A (baseline) vs Option B (optimized)
- [ ] Real performance numbers from production
- [ ] Open-source reference implementation

---

## Lessons for the Research Community

### 1. Measure Before Optimize

**What we did:** Implemented simple version first, will measure, then optimize

**What others often do:** Assume kernel is always faster, optimize prematurely

**Our contribution:** Honest measurements showing when optimization matters

### 2. Separate Critical Path from Analysis

**What we did:** GeoIP in analysis path, not blocking path

**What others often do:** Put everything in one pipeline

**Our contribution:** Clear separation of concerns for latency-sensitive systems

### 3. Feature Placement Driven by Nature, Not Assumptions

**What we did:** Statistical features â†’ userspace (natural fit)

**What others often do:** "eBPF is fast so put everything there"

**Our contribution:** Match feature complexity to execution environment

### 4. Honest TODOs > Pretending Everything Works

**What we did:** Clear TODOs with explanations for 18 features

**What others often do:** Claim all features work, hide limitations

**Our contribution:** Scientific honesty about current limitations

---

## Conclusion

**Decision Summary:**
- âœ… Option A (userspace) for Phase 1
- ðŸ“Š Measure performance baseline
- ðŸ”¬ Publish honest results
- âš¡ Option B (kernel optimization) for Phase 2
- ðŸ“ˆ Publish comparative analysis

**This decision exemplifies:**
- Via Appia engineering (works > perfect)
- Scientific honesty (measure, don't assume)
- Practical systems thinking (critical path separation)
- Iterative development (baseline â†’ optimize)

**Expected publication impact:**
Two papers instead of one:
1. **Phase 1:** "ML Defender: Sub-Microsecond IDS with Synthetic Datasets"
2. **Phase 2:** "Kernel vs Userspace: Performance Trade-offs in Real-Time IDS" (addendum)

---

## References

**Related Work:**
- Cloudflare DDoS protection architecture
- AWS Shield Advanced design
- Cilium eBPF datapath
- XDP performance benchmarks

**Our Contribution:**
Systematic evaluation of feature placement decisions with real measurements from production deployment.

---

**Document Version:** 1.0  
**Author:** Alonso & Claude  
**Philosophy:** Via Appia Quality  
**Motto:** "If your grandmother can't understand it, you haven't explained it clearly enough."
